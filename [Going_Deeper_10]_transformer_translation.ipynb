{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[Going_Deeper_10] transformer_translation",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PsPUTbjVW7qf"
      },
      "outputs": [],
      "source": [
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.font_manager as fm\n",
        "\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        " \n",
        "fontpath = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'\n",
        "font = fm.FontProperties(fname=fontpath, size=9)\n",
        "plt.rc('font', family='NanumBarunGothic') \n",
        "mpl.font_manager.findfont(font)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import re\n",
        "import os\n",
        "import io\n",
        "import time\n",
        "import random\n",
        "\n",
        "import seaborn # Attention 시각화\n",
        "\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "id": "HJfvwZrYXFzQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#파일 가져오기"
      ],
      "metadata": {
        "id": "I6VfOdgGk9pV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kor_path = \"/content/drive/MyDrive/test/data/korean-english-park.train.ko\"\n",
        "eng_path = \"/content/drive/MyDrive/test/data/korean-english-park.train.en\"\n",
        "\n",
        "def clean_corpus(kor_path, eng_path):\n",
        "    with open(kor_path, \"r\") as f: kor = f.read().splitlines()\n",
        "    with open(eng_path, \"r\") as f: eng = f.read().splitlines()\n",
        "    #assert를 사용함 = 뒤 조건이 다를경우 경고메시지 출력. 같으면 오류메시지 띄우지 않음\n",
        "    assert len(kor) == len(eng)\n",
        "\n",
        "    cleaned_corpus = list(set(zip(kor, eng)))  # 중복된 데이터 제거\n",
        "\n",
        "    return cleaned_corpus\n",
        "\n",
        "cleaned_corpus = clean_corpus(kor_path, eng_path)"
      ],
      "metadata": {
        "id": "KOr4OrvCZAVW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#전처리, 토큰화 단계"
      ],
      "metadata": {
        "id": "_4r9M-1ulA7N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_sentence(sentence):\n",
        "\n",
        "    # 소문자변환\n",
        "    sentence = sentence.lower()\n",
        "    # 알파벳, 문장부호, 한글만 남기고 모두 제거\n",
        "    sentence = re.sub(r\"[^a-zA-Z가-힣?.!,]+\", \" \", sentence)\n",
        "    # 문장부호 양옆에 공백 추가\n",
        "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
        "    # 문장 앞뒤의 불필요한 공백 제거\n",
        "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
        "    \n",
        "    return sentence"
      ],
      "metadata": {
        "id": "tHeAGAAXbw96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install sentencepiece"
      ],
      "metadata": {
        "id": "F4oMicmWezhw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece as spm"
      ],
      "metadata": {
        "id": "yOAbxmArgFNH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentencepiece를 활용하여 학습한 tokenizer를 생성\n",
        "def generate_tokenizer(corpus, vocab_size, lang=\"ko\", pad_id=0, bos_id=1, eos_id=2, unk_id=3):\n",
        "    # corpus를 받아 txt로 저장\n",
        "    temp_file = f'/content/drive/MyDrive/test/data/korean-english-park.train.{lang}.txt'\n",
        "    with open(temp_file, 'w') as f:\n",
        "        for row in corpus:\n",
        "            f.write(str(row) +\"\\n\")\n",
        "\n",
        "    spm.SentencePieceTrainer.Train(\n",
        "        f'--input={temp_file} --pad_id={pad_id} --bos_id={bos_id} --eos_id={eos_id} \\\n",
        "        --unk_id={unk_id} --model_prefix=spm_{lang} --vocab_size={vocab_size}'\n",
        "    )\n",
        "    tokenizer = spm.SentencePieceProcessor()\n",
        "    tokenizer.Load(f'spm_{lang}.model')\n",
        "\n",
        "    return tokenizer"
      ],
      "metadata": {
        "id": "nzqo7FKdcgF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_corpus[0]"
      ],
      "metadata": {
        "id": "sBSCEVfriUQt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SRC_VOCAB_SIZE = TGT_VOCAB_SIZE = 20000\n",
        "\n",
        "eng_corpus = []\n",
        "kor_corpus = []\n",
        "\n",
        "for pair in cleaned_corpus:\n",
        "    k, e = pair[0], pair[1]\n",
        "\n",
        "    kor_corpus.append(preprocess_sentence(k))\n",
        "    eng_corpus.append(preprocess_sentence(e))\n",
        "\n",
        "ko_tokenizer = generate_tokenizer(kor_corpus, SRC_VOCAB_SIZE, \"ko\")\n",
        "en_tokenizer = generate_tokenizer(eng_corpus, TGT_VOCAB_SIZE, \"en\")\n",
        "en_tokenizer.set_encode_extra_options(\"bos:eos\")"
      ],
      "metadata": {
        "id": "epEegcvAigWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.notebook import tqdm    # Process 과정을 보기 위해\n",
        "\n",
        "src_corpus = []\n",
        "tgt_corpus = []\n",
        "\n",
        "assert len(kor_corpus) == len(eng_corpus)\n",
        "\n",
        "# 길이가 50 이하인 문장만 남기기\n",
        "for idx in tqdm(range(len(kor_corpus))):\n",
        "    #encodeasids를 통해 [144, 372.....] 와 같은 형태로 인코딩진행\n",
        "    src = ko_tokenizer.EncodeAsIds(kor_corpus[idx])\n",
        "    tgt = en_tokenizer.EncodeAsIds(eng_corpus[idx])\n",
        "\n",
        "    if len(src) <= 50 and len(tgt) <= 50:\n",
        "        src_corpus.append(src)\n",
        "        tgt_corpus.append(tgt)\n",
        "\n",
        "# 패딩처리를 통한 데이터 제작완료\n",
        "enc_train = tf.keras.preprocessing.sequence.pad_sequences(src_corpus, padding='post')\n",
        "dec_train = tf.keras.preprocessing.sequence.pad_sequences(tgt_corpus, padding='post')"
      ],
      "metadata": {
        "id": "452aabMOjH-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#모델 설계"
      ],
      "metadata": {
        "id": "J60Bx6YulIlP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# positional encoding\n",
        "\n",
        "#pos = 단어가 위치한 time-step\n",
        "#d_model = 모델의 embedding 차원 수\n",
        "#i = encoding에서의 index \n",
        "\n",
        "def positional_encoding(pos, d_model):\n",
        "    def cal_angle(position, i):\n",
        "        return position / np.power(10000, int(i) / d_model)\n",
        "\n",
        "    def get_posi_angle_vec(position):\n",
        "        return [cal_angle(position, i) for i in range(d_model)]\n",
        "\n",
        "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(pos)])\n",
        "\n",
        "    #짝수 인덱스에는 sin\n",
        "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])\n",
        "    #홀수 인덱스에는 cos 사용\n",
        "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])\n",
        "\n",
        "    return sinusoid_table"
      ],
      "metadata": {
        "id": "2Y2YOProk4ls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Multi-Head Attention\n",
        "\n",
        "#여러개의 서브모듈을 결합하여 완성.\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "            \n",
        "        self.depth = d_model // self.num_heads\n",
        "            \n",
        "        self.W_q = tf.keras.layers.Dense(d_model)\n",
        "        self.W_k = tf.keras.layers.Dense(d_model)\n",
        "        self.W_v = tf.keras.layers.Dense(d_model)\n",
        "            \n",
        "        self.linear = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "    def scaled_dot_product_attention(self, Q, K, V, mask):\n",
        "        d_k = tf.cast(K.shape[-1], tf.float32)\n",
        "\n",
        "        \"\"\"\n",
        "        Scaled QK 값 구하기\n",
        "        \"\"\"\n",
        "        QK = tf.matmul(Q, K, transpose_b=True)\n",
        "\n",
        "        scaled_qk = QK / tf.math.sqrt(d_k)\n",
        "\n",
        "        if mask is not None: scaled_qk += (mask * -1e9)  \n",
        "\n",
        "        \"\"\"\n",
        "        1. Attention Weights 값 구하기 -> attentions\n",
        "        2. Attention 값을 V에 곱하기 -> out\n",
        "        \"\"\" \n",
        "        attentions = tf.nn.softmax(scaled_qk, axis=-1)\n",
        "        out = tf.matmul(attentions, V)\n",
        "\n",
        "        return out, attentions\n",
        "            \n",
        "\n",
        "    def split_heads(self, x):\n",
        "\n",
        "        \"\"\"\n",
        "        Embedding을 Head의 수로 분할하는 함수\n",
        "\n",
        "        x: [ batch x length x emb ]\n",
        "        return: [ batch x length x heads x self.depth ]\n",
        "        \"\"\"\n",
        "        batch_size = x.shape[0]\n",
        "        split_x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        split_x = tf.transpose(split_x, perm=[0, 2, 1, 3])\n",
        "\n",
        "        return split_x\n",
        "\n",
        "    def combine_heads(self, x):\n",
        "\n",
        "        \"\"\"\n",
        "        분할된 Embedding을 하나로 결합하는 함수\n",
        "\n",
        "        x: [ batch x length x heads x self.depth ]\n",
        "        return: [ batch x length x emb ]\n",
        "        \"\"\"\n",
        "        batch_size = x.shape[0]\n",
        "        combined_x = tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "        combined_x = tf.reshape(combined_x, (batch_size, -1, self.d_model))\n",
        "\n",
        "        return combined_x\n",
        "\n",
        "        \n",
        "    def call(self, Q, K, V, mask):\n",
        "\n",
        "        \"\"\"\n",
        "        Step 1: Linear_in(Q, K, V) -> WQ, WK, WV\n",
        "        Step 2: Split Heads(WQ, WK, WV) -> WQ_split, WK_split, WV_split\n",
        "        Step 3: Scaled Dot Product Attention(WQ_split, WK_split, WV_split)\n",
        "                 -> out, attention_weights\n",
        "        Step 4: Combine Heads(out) -> out\n",
        "        Step 5: Linear_out(out) -> out\n",
        "        \"\"\"\n",
        "        WQ = self.W_q(Q)\n",
        "        WK = self.W_k(K)\n",
        "        WV = self.W_v(V)\n",
        "        \n",
        "        WQ_splits = self.split_heads(WQ)\n",
        "        WK_splits = self.split_heads(WK)\n",
        "        WV_splits = self.split_heads(WV)\n",
        "            \n",
        "        out, attention_weights = self.scaled_dot_product_attention(\n",
        "            WQ_splits, WK_splits, WV_splits, mask)\n",
        "    \t\t\t\t        \n",
        "        out = self.combine_heads(out)\n",
        "        out = self.linear(out)\n",
        "                \n",
        "        return out, attention_weights"
      ],
      "metadata": {
        "id": "8WbB5Ns4mQd8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Position-wise Feed-Forward Network\n",
        "class PoswiseFeedForwardNet(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, d_ff): #\n",
        "        super(PoswiseFeedForwardNet, self).__init__()\n",
        "        self.w_1 = tf.keras.layers.Dense(d_ff, activation='relu')\n",
        "        self.w_2 = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "    def call(self, x):\n",
        "        out = self.w_1(x)\n",
        "        out = self.w_2(out)\n",
        "            \n",
        "        return out"
      ],
      "metadata": {
        "id": "i1A0z1kRmfs2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# encode layer\n",
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        self.enc_self_attn = MultiHeadAttention(d_model, n_heads)\n",
        "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
        "\n",
        "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
        "        \n",
        "    def call(self, x, mask):\n",
        "\n",
        "        \"\"\"\n",
        "        Multi-Head Attention\n",
        "        \"\"\"\n",
        "        residual = x\n",
        "        out = self.norm_1(x)\n",
        "        out, enc_attn = self.enc_self_attn(out, out, out, mask)\n",
        "        out = self.dropout(out)\n",
        "        out += residual\n",
        "        \n",
        "        \"\"\"\n",
        "        Position-Wise Feed Forward Network\n",
        "        \"\"\"\n",
        "        residual = out\n",
        "        out = self.norm_2(out)\n",
        "        out = self.ffn(out)\n",
        "        out = self.dropout(out)\n",
        "        out += residual\n",
        "        \n",
        "        return out, enc_attn"
      ],
      "metadata": {
        "id": "SKr4RixTmlEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# decode layer\n",
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        self.dec_self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
        "\n",
        "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.norm_3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
        "    \n",
        "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
        "\n",
        "        \"\"\"\n",
        "        Masked Multi-Head Attention\n",
        "        \"\"\"\n",
        "        residual = x\n",
        "        out = self.norm_1(x)\n",
        "        out, dec_attn = self.dec_self_attn(out, out, out, padding_mask)\n",
        "        out = self.dropout(out)\n",
        "        out += residual\n",
        "\n",
        "        \"\"\"\n",
        "        Multi-Head Attention\n",
        "        \"\"\"\n",
        "        residual = out\n",
        "        out = self.norm_2(out)\n",
        "        out, dec_enc_attn = self.enc_dec_attn(out, enc_out, enc_out, causality_mask)\n",
        "        out = self.dropout(out)\n",
        "        out += residual\n",
        "        \n",
        "        \"\"\"\n",
        "        Position-Wise Feed Forward Network\n",
        "        \"\"\"\n",
        "        residual = out\n",
        "        out = self.norm_3(out)\n",
        "        out = self.ffn(out)\n",
        "        out = self.dropout(out)\n",
        "        out += residual\n",
        "\n",
        "        return out, dec_attn, dec_enc_attn"
      ],
      "metadata": {
        "id": "BJiUz1ZrmqAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self,\n",
        "                 n_layers,\n",
        "                 d_model,\n",
        "                 n_heads,\n",
        "                 d_ff,\n",
        "                 dropout):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.n_layers = n_layers\n",
        "        self.enc_layers = [EncoderLayer(d_model, n_heads, d_ff, dropout) \n",
        "                        for _ in range(n_layers)]\n",
        "        \n",
        "    def call(self, x, mask):\n",
        "        out = x\n",
        "    \n",
        "        enc_attns = list()\n",
        "        for i in range(self.n_layers):\n",
        "            out, enc_attn = self.enc_layers[i](out, mask)\n",
        "            enc_attns.append(enc_attn)\n",
        "        \n",
        "        return out, enc_attns"
      ],
      "metadata": {
        "id": "oDjRy2EYms5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self,\n",
        "                 n_layers,\n",
        "                 d_model,\n",
        "                 n_heads,\n",
        "                 d_ff,\n",
        "                 dropout):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.n_layers = n_layers\n",
        "        self.dec_layers = [DecoderLayer(d_model, n_heads, d_ff, dropout) \n",
        "                            for _ in range(n_layers)]\n",
        "                            \n",
        "                            \n",
        "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
        "        out = x\n",
        "    \n",
        "        dec_attns = list()\n",
        "        dec_enc_attns = list()\n",
        "        for i in range(self.n_layers):\n",
        "            out, dec_attn, dec_enc_attn = \\\n",
        "            self.dec_layers[i](out, enc_out, causality_mask, padding_mask)\n",
        "\n",
        "            dec_attns.append(dec_attn)\n",
        "            dec_enc_attns.append(dec_enc_attn)\n",
        "\n",
        "        return out, dec_attns, dec_enc_attns"
      ],
      "metadata": {
        "id": "Y6hEJSrqmv9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# transformer부분\n",
        "\n",
        "class Transformer(tf.keras.Model):\n",
        "    def __init__(self,\n",
        "                    n_layers,\n",
        "                    d_model,\n",
        "                    n_heads,\n",
        "                    d_ff,\n",
        "                    src_vocab_size,\n",
        "                    tgt_vocab_size,\n",
        "                    pos_len,\n",
        "                    dropout=0.2,\n",
        "                    shared=True):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.d_model = tf.cast(d_model, tf.float32)\n",
        "\n",
        "        \"\"\"\n",
        "        1. Embedding Layer 정의\n",
        "        2. Positional Encoding 정의\n",
        "        3. Encoder / Decoder 정의\n",
        "        4. Output Linear 정의\n",
        "        5. Shared Weights\n",
        "        6. Dropout 정의\n",
        "        \"\"\"\n",
        "        self.enc_emb = tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
        "        self.dec_emb = tf.keras.layers.Embedding(tgt_vocab_size, d_model)\n",
        "\n",
        "        self.pos_encoding = positional_encoding(pos_len, d_model)\n",
        "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
        "\n",
        "        self.encoder = Encoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
        "        self.decoder = Decoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
        "\n",
        "        self.fc = tf.keras.layers.Dense(tgt_vocab_size)\n",
        "\n",
        "        self.shared = shared\n",
        "\n",
        "        if shared: self.fc.set_weights(tf.transpose(self.dec_emb.weights))\n",
        "\n",
        "    def embedding(self, emb, x):\n",
        "\n",
        "        \"\"\"\n",
        "        입력된 정수 배열을 Embedding + Pos Encoding\n",
        "        + Shared일 경우 Scaling 작업 포함\n",
        "\n",
        "        x: [ batch x length ]\n",
        "        return: [ batch x length x emb ]\n",
        "        \"\"\"\n",
        "        seq_len = x.shape[1]\n",
        "        out = emb(x)\n",
        "\n",
        "        if self.shared: out *= tf.math.sqrt(self.d_model)\n",
        "\n",
        "        out += self.pos_encoding[np.newaxis, ...][:, :seq_len, :]\n",
        "        out = self.dropout(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "        \n",
        "    def call(self, enc_in, dec_in, enc_mask, causality_mask, dec_mask):\n",
        "        \n",
        "        \"\"\"\n",
        "        Step 1: Embedding(enc_in, dec_in) -> enc_in, dec_in\n",
        "        Step 2: Encoder(enc_in, enc_mask) -> enc_out, enc_attns\n",
        "        Step 3: Decoder(dec_in, enc_out, mask)\n",
        "                -> dec_out, dec_attns, dec_enc_attns\n",
        "        Step 4: Out Linear(dec_out) -> logits\n",
        "        \"\"\"\n",
        "        enc_in = self.embedding(self.enc_emb, enc_in)\n",
        "        dec_in = self.embedding(self.dec_emb, dec_in)\n",
        "\n",
        "        enc_out, enc_attns = self.encoder(enc_in, enc_mask)\n",
        "        \n",
        "        dec_out, dec_attns, dec_enc_attns = \\\n",
        "        self.decoder(dec_in, enc_out, causality_mask, dec_mask)\n",
        "        \n",
        "        logits = self.fc(dec_out)\n",
        "        \n",
        "        return logits, enc_attns, dec_attns, dec_enc_attns"
      ],
      "metadata": {
        "id": "GzH0RaH2mw_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mask 부분\n",
        "def generate_padding_mask(seq):\n",
        "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "def generate_causality_mask(src_len, tgt_len):\n",
        "    mask = 1 - np.cumsum(np.eye(src_len, tgt_len), 0)\n",
        "    return tf.cast(mask, tf.float32)\n",
        "\n",
        "def generate_masks(src, tgt):\n",
        "    enc_mask = generate_padding_mask(src)\n",
        "    dec_mask = generate_padding_mask(tgt)\n",
        "\n",
        "    dec_enc_causality_mask = generate_causality_mask(tgt.shape[1], src.shape[1])\n",
        "    dec_enc_mask = tf.maximum(enc_mask, dec_enc_causality_mask)\n",
        "\n",
        "    dec_causality_mask = generate_causality_mask(tgt.shape[1], tgt.shape[1])\n",
        "    dec_mask = tf.maximum(dec_mask, dec_causality_mask)\n",
        "\n",
        "    return enc_mask, dec_enc_mask, dec_mask"
      ],
      "metadata": {
        "id": "Ux_zUmCHm8Zd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#모델훈련"
      ],
      "metadata": {
        "id": "K9MdCZTynYaS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transformer = Transformer(n_layers=2, d_model=512, n_heads=8, d_ff=1024, src_vocab_size=SRC_VOCAB_SIZE,\n",
        "    tgt_vocab_size=TGT_VOCAB_SIZE, pos_len=200, dropout=0.1, shared=True)"
      ],
      "metadata": {
        "id": "7lzxmfLKnJg4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(LearningRateScheduler, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.warmup_steps = warmup_steps\n",
        "    \n",
        "    def __call__(self, step):\n",
        "        arg1 = step ** -0.5\n",
        "        arg2 = step * (self.warmup_steps ** -1.5)\n",
        "        \n",
        "        return (self.d_model ** -0.5) * tf.math.minimum(arg1, arg2)\n",
        "\n",
        "learning_rate = LearningRateScheduler(512)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate,\n",
        "                                     beta_1=0.9,\n",
        "                                     beta_2=0.98, \n",
        "                                     epsilon=1e-9)"
      ],
      "metadata": {
        "id": "SQ652NihpIeg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#loss 함수 정의\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    # Masking 되지 않은 입력의 개수로 Scaling하는 과정\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
      ],
      "metadata": {
        "id": "pMUaNb_DpXg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Step 함수 정의\n",
        "\n",
        "@tf.function()\n",
        "def train_step(src, tgt, model, optimizer):\n",
        "    gold = tgt[:, 1:]\n",
        "        \n",
        "    enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt)\n",
        "\n",
        "    # 계산된 loss에 tf.GradientTape()를 적용해 학습진행\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions, enc_attns, dec_attns, dec_enc_attns = \\\n",
        "        model(src, tgt, enc_mask, dec_enc_mask, dec_mask)\n",
        "        loss = loss_function(gold, predictions[:, :-1])\n",
        "\n",
        "    # 최종적으로 optimizer.apply_gradients()가 사용됩니다. \n",
        "    gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
        "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "    \n",
        "    return loss, enc_attns, dec_attns, dec_enc_attns"
      ],
      "metadata": {
        "id": "8zwToMCDpfzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Attention 시각화 함수\n",
        "\n",
        "def visualize_attention(src, tgt, enc_attns, dec_attns, dec_enc_attns):\n",
        "    def draw(data, ax, x=\"auto\", y=\"auto\"):\n",
        "        import seaborn\n",
        "        seaborn.heatmap(data, \n",
        "                        square=True,\n",
        "                        vmin=0.0, vmax=1.0, \n",
        "                        cbar=False, ax=ax,\n",
        "                        xticklabels=x,\n",
        "                        yticklabels=y)\n",
        "        \n",
        "    for layer in range(0, 2, 1):\n",
        "        fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n",
        "        print(\"Encoder Layer\", layer + 1)\n",
        "        for h in range(4):\n",
        "            draw(enc_attns[layer][0, h, :len(src), :len(src)], axs[h], src, src)\n",
        "        plt.show()\n",
        "        \n",
        "    for layer in range(0, 2, 1):\n",
        "        fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n",
        "        print(\"Decoder Self Layer\", layer+1)\n",
        "        for h in range(4):\n",
        "            draw(dec_attns[layer][0, h, :len(tgt), :len(tgt)], axs[h], tgt, tgt)\n",
        "        plt.show()\n",
        "\n",
        "        print(\"Decoder Src Layer\", layer+1)\n",
        "        fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n",
        "        for h in range(4):\n",
        "            draw(dec_enc_attns[layer][0, h, :len(tgt), :len(src)], axs[h], src, tgt)\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "JHm8foResn0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 번역 생성 함수\n",
        "\n",
        "def evaluate(sentence, model, src_tokenizer, tgt_tokenizer):\n",
        "    sentence = preprocess_sentence(sentence)\n",
        "\n",
        "    pieces = src_tokenizer.encode_as_pieces(sentence)\n",
        "    tokens = src_tokenizer.encode_as_ids(sentence)\n",
        "\n",
        "    _input = tf.keras.preprocessing.sequence.pad_sequences([tokens],\n",
        "                                                           maxlen=enc_train.shape[-1],\n",
        "                                                           padding='post')\n",
        "    \n",
        "    ids = []\n",
        "    output = tf.expand_dims([tgt_tokenizer.bos_id()], 0)\n",
        "    for i in range(dec_train.shape[-1]):\n",
        "        enc_padding_mask, combined_mask, dec_padding_mask = \\\n",
        "        generate_masks(_input, output)\n",
        "\n",
        "        predictions, enc_attns, dec_attns, dec_enc_attns =\\\n",
        "        model(_input, \n",
        "              output,\n",
        "              enc_padding_mask,\n",
        "              combined_mask,\n",
        "              dec_padding_mask)\n",
        "\n",
        "        predicted_id = \\\n",
        "        tf.argmax(tf.math.softmax(predictions, axis=-1)[0, -1]).numpy().item()\n",
        "\n",
        "        if tgt_tokenizer.eos_id() == predicted_id:\n",
        "            result = tgt_tokenizer.decode_ids(ids)\n",
        "            return pieces, result, enc_attns, dec_attns, dec_enc_attns\n",
        "\n",
        "        ids.append(predicted_id)\n",
        "        output = tf.concat([output, tf.expand_dims([predicted_id], 0)], axis=-1)\n",
        "\n",
        "    result = tgt_tokenizer.decode_ids(ids)\n",
        "\n",
        "    return pieces, result, enc_attns, dec_attns, dec_enc_attns"
      ],
      "metadata": {
        "id": "030HjLjgtzK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 번역 생성 및 Attention 시각화 결합\n",
        "\n",
        "def translate(sentence, model, src_tokenizer, tgt_tokenizer, plot_attention=False):\n",
        "    pieces, result, enc_attns, dec_attns, dec_enc_attns = \\\n",
        "    evaluate(sentence, model, src_tokenizer, tgt_tokenizer)\n",
        "    \n",
        "    print('Input: %s' % (sentence))\n",
        "    print('Predicted translation: {}'.format(result))\n",
        "\n",
        "    if plot_attention:\n",
        "        visualize_attention(pieces, result.split(), enc_attns, dec_attns, dec_enc_attns)"
      ],
      "metadata": {
        "id": "VzDUBQost0D8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm_notebook \n",
        "\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 25\n",
        "\n",
        "examples = [\n",
        "            \"오바마는 대통령이다.\",\n",
        "            \"시민들은 도시 속에 산다.\",\n",
        "            \"커피는 필요 없다.\",\n",
        "            \"일곱 명의 사망자가 발생했다.\"\n",
        "]\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    total_loss = 0\n",
        "    \n",
        "    idx_list = list(range(0, enc_train.shape[0], BATCH_SIZE))\n",
        "    random.shuffle(idx_list)\n",
        "    t = tqdm_notebook(idx_list)\n",
        "\n",
        "    for (batch, idx) in enumerate(t):\n",
        "        batch_loss, enc_attns, dec_attns, dec_enc_attns = \\\n",
        "        train_step(enc_train[idx:idx+BATCH_SIZE],\n",
        "                    dec_train[idx:idx+BATCH_SIZE],\n",
        "                    transformer,\n",
        "                    optimizer)\n",
        "\n",
        "        total_loss += batch_loss\n",
        "        \n",
        "        t.set_description_str('Epoch %2d' % (epoch + 1))\n",
        "        t.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (batch + 1)))\n",
        "\n",
        "    for example in examples:\n",
        "        translate(example, transformer, ko_tokenizer, en_tokenizer)"
      ],
      "metadata": {
        "id": "cyWKwPHZt2gy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "중간중간 잘 번역이 되었다가도 다음 스텝으로 넘어갈 때마다 이상한 결과를 도출하기도 한다. 하이퍼 파라미터를 변경하여 다시 수행하고자 한다"
      ],
      "metadata": {
        "id": "iPMH6oq6_IdO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transformer2 = Transformer(\n",
        "    n_layers=4,\n",
        "    d_model=512,\n",
        "    n_heads=8,\n",
        "    d_ff=2048,\n",
        "    src_vocab_size=SRC_VOCAB_SIZE,\n",
        "    tgt_vocab_size=TGT_VOCAB_SIZE,\n",
        "    pos_len=200,\n",
        "    dropout=0.2,\n",
        "    shared=True\n",
        ")"
      ],
      "metadata": {
        "id": "pgXjSUJyuK2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Step 함수 정의\n",
        "\n",
        "@tf.function()\n",
        "def train_step2(src, tgt, model, optimizer):\n",
        "    gold = tgt[:, 1:]\n",
        "        \n",
        "    enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt)\n",
        "\n",
        "    # 계산된 loss에 tf.GradientTape()를 적용해 학습진행\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions, enc_attns, dec_attns, dec_enc_attns = \\\n",
        "        model(src, tgt, enc_mask, dec_enc_mask, dec_mask)\n",
        "        loss = loss_function(gold, predictions[:, :-1])\n",
        "\n",
        "    # 최종적으로 optimizer.apply_gradients()가 사용됩니다. \n",
        "    gradients = tape.gradient(loss, transformer2.trainable_variables)    \n",
        "    optimizer.apply_gradients(zip(gradients, transformer2.trainable_variables))\n",
        "    \n",
        "    return loss, enc_attns, dec_attns, dec_enc_attns"
      ],
      "metadata": {
        "id": "j02ySv_W9PWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 25\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    total_loss = 0\n",
        "\n",
        "    idx_list = list(range(0, enc_train.shape[0], BATCH_SIZE))\n",
        "    random.shuffle(idx_list)\n",
        "    t = tqdm(idx_list)\n",
        "\n",
        "    for (batch, idx) in enumerate(t):\n",
        "        batch_loss, enc_attns, dec_attns, dec_enc_attns = train_step2(enc_train[idx:idx+BATCH_SIZE],\n",
        "                                                                     dec_train[idx:idx+BATCH_SIZE],\n",
        "                                                                     transformer2,\n",
        "                                                                     optimizer)\n",
        "\n",
        "        total_loss += batch_loss\n",
        "\n",
        "        t.set_description_str('Epoch %2d' % (epoch + 1))\n",
        "        t.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (batch + 1)))\n",
        "    \n",
        "    # 매 Epoch 마다 제시된 예문에 대한 번역 생성\n",
        "    for example in examples:\n",
        "        translate(example, transformer2, ko_tokenizer, en_tokenizer, True)"
      ],
      "metadata": {
        "id": "5RYyzdTu3gT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 회고\n",
        "* 레이어를 2개만 사용한 것과, 4개 사용한 것 모두 좋은 결과를 보여주지는 못하였다.\n",
        "- 논문에서의 트랜스포머 인코더, 디코더 층이 6개인 것을 봤을 때, 층을 더 늘리면 더 좋은 결과를 얻을 수 있을 것으로 보인다\n",
        "- epoch를 더 늘릴 경우 더 좋은 결과를 얻을 수 있을 것이라 생각하지만 상상 이상으로 많은 epoch값이 필요할 것으로 보인다.\n",
        "- 올바른 번역을 하였지만 훈련 횟수가 많아질수록 올바르지 못한 번역으로 바뀌는 것이 많이 보인다. 배치사이즈가 너무 작은 것이 문제인 것 같다(128까지는 늘릴 수 있을지도 모르겠으나, 256부터는 커널이 죽어버리는 경우가 발생하여서 함부로 시도하기엔 어려운 사항이 있다.\n",
        "- 모르는 부분이 너무 많아서 포르투갈어/영어 트랜스포머 코드를 가져왔다. cnn논문을 시작으로 하나씩 읽어보고 구현 방법을 조사해야겠다."
      ],
      "metadata": {
        "id": "FiqQnE1u_rfL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "kP3EY_D03q4d"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}