{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46079b38",
   "metadata": {},
   "source": [
    "# 데이터 읽어오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb00dcaf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " [\"Now I've heard there was a secret chord\", 'That David played, and it pleased the Lord', \"But you don't really care for music, do you?\"]\n"
     ]
    }
   ],
   "source": [
    "import os, re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "\n",
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "#여러 파일을 읽고 raw_corpus에 담기\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "        \n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6693d5",
   "metadata": {},
   "source": [
    "# 데이터 정제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fde66a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now I've heard there was a secret chord\n",
      "That David played, and it pleased the Lord\n",
      "But you don't really care for music, do you?\n",
      "It goes like this\n",
      "The fourth, the fifth\n",
      "The minor fall, the major lift\n",
      "The baffled king composing Hallelujah Hallelujah\n",
      "Hallelujah\n",
      "Hallelujah\n",
      "Hallelujah Your faith was strong but you needed proof\n",
      "You saw her bathing on the roof\n",
      "Her beauty and the moonlight overthrew her\n",
      "She tied you\n",
      "To a kitchen chair\n",
      "She broke your throne, and she cut your hair\n",
      "And from your lips she drew the Hallelujah Hallelujah\n"
     ]
    }
   ],
   "source": [
    "for idx, sentence in enumerate(raw_corpus):\n",
    "    if len(sentence) == 0: continue      #길이 0인문장은 건너뜀\n",
    "    if sentence[-1] == \":\": continue     #문장끝이 :인 문장은 건너뛸 것\n",
    "        \n",
    "    if idx>15: break  #길이 15초과일경우 반복문 종료\n",
    "        \n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3145fa",
   "metadata": {},
   "source": [
    "# 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b549ca0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> this ! i ¿ s samp ! le <end>\n"
     ]
    }
   ],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip() #소문자로 바꾸고 양쪽공백 지움\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) #특수문자 양쪽에 공백\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) #여러개의 공백은 하나의 공백으로\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) #a-z 등등 이 아닌 문자를 공백으로 바꿈\n",
    "    sentence = sentence.strip() # 다시 양쪽 공백을 제거\n",
    "    sentence = '<start> ' + sentence + ' <end>' #문장 시작에는 <start>, 끝은 <end>추가\n",
    "    return sentence\n",
    "\n",
    "print(preprocess_sentence(\"@this! i¿s          samp!le\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94155a03",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> now i ve heard there was a secret chord <end>',\n",
       " '<start> that david played , and it pleased the lord <end>',\n",
       " '<start> but you don t really care for music , do you ? <end>',\n",
       " '<start> it goes like this <end>',\n",
       " '<start> the fourth , the fifth <end>',\n",
       " '<start> the minor fall , the major lift <end>',\n",
       " '<start> the baffled king composing hallelujah hallelujah <end>',\n",
       " '<start> hallelujah <end>',\n",
       " '<start> hallelujah <end>',\n",
       " '<start> hallelujah your faith was strong but you needed proof <end>',\n",
       " '<start> you saw her bathing on the roof <end>',\n",
       " '<start> her beauty and the moonlight overthrew her <end>',\n",
       " '<start> she tied you <end>',\n",
       " '<start> to a kitchen chair <end>',\n",
       " '<start> she broke your throne , and she cut your hair <end>']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [] #정제된 문장 모음\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    if len(sentence) == 0: continue    #원하지 않는 문장 생략\n",
    "    if sentence[-1] == \":\": continue  #대본의 경우 마지막에 \":\" 이 붙음\n",
    "        \n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    corpus.append(preprocessed_sentence)\n",
    "    \n",
    "corpus[:15] #정제된 것 15개 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752220e8",
   "metadata": {},
   "source": [
    "# 토큰화과정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc6554f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2   50    5 ...    0    0    0]\n",
      " [   2   17 2639 ...    0    0    0]\n",
      " [   2   36    7 ...   43    3    0]\n",
      " ...\n",
      " [   5   22    9 ...   10 1013    3]\n",
      " [  37   15 9049 ...  877  647    3]\n",
      " [   2    7   34 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7faa7a4bccd0>\n"
     ]
    }
   ],
   "source": [
    "def tokenize(corpus):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "    num_words=12000, \n",
    "    filters=' ',\n",
    "    oov_token=\"<unk>\"  # 12,000단어에 포함되지 못한 단어는 '<unk> =unknown으로 바꿈\n",
    "    )                  #oov_token은 어휘목록에 포함되지 않은 단어를 대체할 때 사용\n",
    "    \n",
    "    tokenizer.fit_on_texts(corpus) #corpus를 이용해 tokenizer내부의 단어장 완성\n",
    "    tensor = tokenizer.texts_to_sequences(corpus) #tokenizer를 이용해 corpus를 tensor로 변환\n",
    "    #texts_to_sequences 메서드는 텍스트 안의 단어들을 숫자의 시퀀스로 변형시켜주는 것\n",
    "    \n",
    "    #입력 데이터의 시퀀스 길이를 맞춤, 시퀀스가 짧다면 문장 뒤에 패딩을 붙여 길이를 맞춤\n",
    "    #문장 앞에 패딩을 붙여 길이를 맞추고싶다면 padding='pre'를 사용\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, maxlen=15, padding='post')\n",
    "    \n",
    "    print(tensor, tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b403963d",
   "metadata": {},
   "source": [
    "###### 이 부분에서 아무런 이상이 없는 줄 알았는데, maxlen 값을 하지 않아서 학습단계에서 아무것도 진행할 수 없었다. 영어라고 대충 읽지 말고 꼼꼼히 읽어서 이런 일이 일어나지 않도록 해야겠다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71e75e3",
   "metadata": {},
   "source": [
    "# 단어 구축 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64c65130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : ,\n",
      "5 : i\n",
      "6 : the\n",
      "7 : you\n",
      "8 : and\n",
      "9 : a\n",
      "10 : to\n",
      "11 : it\n",
      "12 : me\n",
      "13 : my\n",
      "14 : in\n",
      "15 : t\n"
     ]
    }
   ],
   "source": [
    "for idx in tokenizer.index_word:      #단어확인\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "    \n",
    "    if idx>=15: break     #단어장 중 15번째의 단어까지 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69b3b5a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   2   50    5   91  297   65   57    9  969 6042    3    0    0    0]\n",
      "[  50    5   91  297   65   57    9  969 6042    3    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "src_input = tensor[:, :-1]  #마지막 부분을 잘라내서 문장 생성 = 패딩일 확률 높음\n",
    "tgt_input = tensor[:, 1:]   #0번째 부터가 아닌 첫번째 부터 진행 = start를 자르기 위함\n",
    "\n",
    "                            #패딩이 할당된 값은 0이 있기는 하지만,의미를 가지지 않음\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42deadce",
   "metadata": {},
   "source": [
    "###### src_input = tensor[:, :-1] 부분은 end를 잘라내기 위함이라고 생각할 수있으나 pad값일 가능성이 높기 때문에 가장 마지막부분인 -1을 제외\n",
    "###### tensor[:, :-1] 부분에서 콤마 앞부분의 콜론은 해당 문장을 선택한다는 뜻으로 보면 됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce52da5",
   "metadata": {},
   "source": [
    "###### 패딩이라는 단어가 꽤 생소해서 이것이 어떻게 작동하나 찾아봤는데, 데이터의 크기를 맞추기 위해 특정 값을 채우는 것을 패딩이라고 한다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03680514",
   "metadata": {},
   "source": [
    "###### src_input = tensor[:, :-1] 이 부분을 -1에서 자를 필요가 없이 범위를 지정하지 않으면 된다고 생각했으나, 팀원 분들께 여쭤보니 데이터 사이즈를 똑같이 맞추는데 중요한 역할을 한다고 하셨음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a18a1be",
   "metadata": {},
   "source": [
    "# 데이터 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69865023",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(\n",
    "    src_input,\n",
    "    tgt_input,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f424a2a8",
   "metadata": {},
   "source": [
    "# 데이터셋 객체 형성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e71a851b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 14), (256, 14)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE= 256                #BATCH_SIZE= 한번의 BATCH마다 주는 데이터 샘플의 SIZE\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    " # tokenizer가 구축한 단어 + 0:<pad>를 포함하여 12,001개의 값을 가짐\n",
    "VOCAB_SIZE = tokenizer.num_words + 1\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input))\n",
    "dataset= dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d57251c",
   "metadata": {},
   "source": [
    "#### 데이터셋을 얻음으로써 데이터 다듬기 과정 끝"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997875e4",
   "metadata": {},
   "source": [
    "# 모델만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd8b8684",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size=256    #워드 벡터의 차원수, 단어가 추상적으로 표현되는 크기 [0.0, 1.0] 등을 의미\n",
    "hidden_size = 1024   #모델에 얼마나 많은 일꾼을 둘 것인가?\n",
    "model = TextGenerator(tokenizer.num_words+1, embedding_size, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5af139fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 14, 12001), dtype=float32, numpy=\n",
       "array([[[ 9.36345605e-05,  1.48246865e-04, -1.43573998e-04, ...,\n",
       "         -2.89032410e-04, -3.02881963e-04,  5.74998267e-05],\n",
       "        [ 1.31555398e-05,  2.61100591e-04, -2.20084083e-04, ...,\n",
       "         -5.37787098e-04, -5.73951635e-04,  2.02090487e-05],\n",
       "        [-1.40361444e-04,  3.51603754e-04, -2.41852744e-04, ...,\n",
       "         -9.27612185e-04, -8.35872896e-04,  7.22916157e-05],\n",
       "        ...,\n",
       "        [ 5.21681679e-04, -1.63337623e-03,  2.77033367e-04, ...,\n",
       "          1.93671859e-03,  5.73755067e-04, -3.03467666e-03],\n",
       "        [ 5.42365015e-04, -1.78395933e-03,  2.93628458e-04, ...,\n",
       "          2.06568115e-03,  6.72652735e-04, -3.25790583e-03],\n",
       "        [ 5.59798733e-04, -1.91784464e-03,  2.99380743e-04, ...,\n",
       "          2.15961062e-03,  7.44132267e-04, -3.44165321e-03]],\n",
       "\n",
       "       [[ 9.36345605e-05,  1.48246865e-04, -1.43573998e-04, ...,\n",
       "         -2.89032410e-04, -3.02881963e-04,  5.74998267e-05],\n",
       "        [-2.91160868e-05,  3.99695913e-04, -2.55020306e-04, ...,\n",
       "         -5.61819354e-04, -5.28412522e-04, -9.59244571e-05],\n",
       "        [ 9.71132467e-05,  6.56305696e-04, -3.02693923e-04, ...,\n",
       "         -4.69587627e-04, -6.69187575e-04, -4.39466567e-05],\n",
       "        ...,\n",
       "        [ 4.11733345e-04,  1.85961952e-04,  6.11381838e-04, ...,\n",
       "          2.72495963e-04,  7.91655984e-05,  1.50513474e-03],\n",
       "        [ 2.92982586e-04, -1.57789269e-04,  5.78738051e-04, ...,\n",
       "          4.47803293e-04,  2.18022183e-06,  1.46016246e-03],\n",
       "        [ 2.88874464e-04, -5.56957035e-04,  5.14808111e-04, ...,\n",
       "          7.79257505e-04,  2.13382955e-05,  1.12865353e-03]],\n",
       "\n",
       "       [[ 9.36345605e-05,  1.48246865e-04, -1.43573998e-04, ...,\n",
       "         -2.89032410e-04, -3.02881963e-04,  5.74998267e-05],\n",
       "        [ 9.82064084e-05,  1.25579609e-04, -1.75780428e-04, ...,\n",
       "         -6.39412901e-04, -6.03244989e-04,  2.44025228e-04],\n",
       "        [-1.46171849e-04,  3.49878646e-05, -2.12749277e-04, ...,\n",
       "         -7.69422739e-04, -7.89441227e-04,  5.33669721e-04],\n",
       "        ...,\n",
       "        [ 3.18618026e-04, -8.96457117e-04, -6.52859017e-05, ...,\n",
       "          1.65645930e-03,  6.20561768e-05, -1.37937686e-03],\n",
       "        [ 4.36015689e-04, -1.07788725e-03, -3.12987104e-05, ...,\n",
       "          1.86375505e-03,  2.48024327e-04, -1.85695314e-03],\n",
       "        [ 5.21758862e-04, -1.25233771e-03,  7.19455215e-07, ...,\n",
       "          2.01194664e-03,  4.11844667e-04, -2.28463695e-03]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 9.36345605e-05,  1.48246865e-04, -1.43573998e-04, ...,\n",
       "         -2.89032410e-04, -3.02881963e-04,  5.74998267e-05],\n",
       "        [ 3.48622561e-04,  3.60125210e-04,  1.89686441e-04, ...,\n",
       "         -6.20609440e-04, -3.33081698e-04,  3.88526169e-05],\n",
       "        [ 7.36091461e-04,  4.54909954e-04,  5.43199887e-04, ...,\n",
       "         -7.03222468e-04, -3.63253144e-04, -1.38677729e-04],\n",
       "        ...,\n",
       "        [-9.06010799e-04,  1.24355822e-04,  7.28789018e-04, ...,\n",
       "         -4.89031372e-04, -3.66298162e-04,  3.28391441e-04],\n",
       "        [-7.19981093e-04, -2.32683538e-04,  4.69338993e-04, ...,\n",
       "         -1.63898469e-04, -3.51245952e-04,  4.12372065e-05],\n",
       "        [-4.92538849e-04, -5.81924105e-04,  2.83450383e-04, ...,\n",
       "          2.21426890e-04, -2.46585958e-04, -3.72133567e-04]],\n",
       "\n",
       "       [[ 9.36345605e-05,  1.48246865e-04, -1.43573998e-04, ...,\n",
       "         -2.89032410e-04, -3.02881963e-04,  5.74998267e-05],\n",
       "        [ 2.43988528e-04,  2.13241641e-04, -1.85063982e-04, ...,\n",
       "         -2.82482302e-04, -2.90463242e-04, -1.65176112e-04],\n",
       "        [ 1.01282334e-04,  1.67244230e-04,  7.98475739e-05, ...,\n",
       "         -2.72108591e-04, -2.55134946e-04, -2.14239742e-04],\n",
       "        ...,\n",
       "        [-6.73614340e-05, -9.79443546e-04,  2.73494108e-04, ...,\n",
       "          6.14433200e-04, -5.12028928e-04, -5.91176096e-04],\n",
       "        [ 4.85203418e-05, -1.20747299e-03,  2.84288777e-04, ...,\n",
       "          9.49071487e-04, -2.87595845e-04, -1.02775311e-03],\n",
       "        [ 1.48107021e-04, -1.41446036e-03,  2.98196741e-04, ...,\n",
       "          1.23528962e-03, -6.11848809e-05, -1.45587244e-03]],\n",
       "\n",
       "       [[ 9.36345605e-05,  1.48246865e-04, -1.43573998e-04, ...,\n",
       "         -2.89032410e-04, -3.02881963e-04,  5.74998267e-05],\n",
       "        [ 4.51107015e-04,  2.71527933e-05, -8.31420693e-05, ...,\n",
       "         -4.15422517e-04, -6.95812982e-04, -2.34504536e-04],\n",
       "        [ 4.33869980e-04, -1.22177065e-04, -2.38257751e-04, ...,\n",
       "         -4.15221089e-04, -9.48868692e-04, -2.69735523e-04],\n",
       "        ...,\n",
       "        [-5.51071480e-06, -1.47216092e-03, -2.36161781e-04, ...,\n",
       "          1.13295845e-03, -6.31793053e-04, -1.28242548e-03],\n",
       "        [ 6.56074917e-05, -1.62204192e-03, -1.68304061e-04, ...,\n",
       "          1.45618839e-03, -3.73076880e-04, -1.67337852e-03],\n",
       "        [ 1.27413587e-04, -1.75997987e-03, -1.10656249e-04, ...,\n",
       "          1.71765662e-03, -1.39658907e-04, -2.03574146e-03]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#데이터셋에서 데이터 한 배치만 불러오는 방법\n",
    "for src_sample, tgt_sample in dataset.take(1): break\n",
    "    \n",
    "model(src_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e70472a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  3072256   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  multiple                  5246976   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                multiple                  8392704   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  12301025  \n",
      "=================================================================\n",
      "Total params: 29,012,961\n",
      "Trainable params: 29,012,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182d51e2",
   "metadata": {},
   "source": [
    "# 학습중"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "669a2654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "686/686 [==============================] - 122s 174ms/step - loss: 3.6126\n",
      "Epoch 2/10\n",
      "686/686 [==============================] - 119s 174ms/step - loss: 3.1393\n",
      "Epoch 3/10\n",
      "686/686 [==============================] - 120s 174ms/step - loss: 2.9478\n",
      "Epoch 4/10\n",
      "686/686 [==============================] - 120s 174ms/step - loss: 2.7997\n",
      "Epoch 5/10\n",
      "686/686 [==============================] - 119s 174ms/step - loss: 2.6711\n",
      "Epoch 6/10\n",
      "686/686 [==============================] - 119s 174ms/step - loss: 2.5539\n",
      "Epoch 7/10\n",
      "686/686 [==============================] - 119s 174ms/step - loss: 2.4452\n",
      "Epoch 8/10\n",
      "686/686 [==============================] - 119s 174ms/step - loss: 2.3428\n",
      "Epoch 9/10\n",
      "686/686 [==============================] - 120s 174ms/step - loss: 2.2465\n",
      "Epoch 10/10\n",
      "686/686 [==============================] - 120s 174ms/step - loss: 2.1553\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa9fab224c0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae3bb03",
   "metadata": {},
   "source": [
    "### 이 부분에서 에러가나서 3시간 넘게 아무것도 하지 못했었다. 조원분들과 토론 결과 토큰 갯수를 15개 이하로 설정해야하는 것을 알게 되었다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e46b2d1",
   "metadata": {},
   "source": [
    "# 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "18e60457",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 텐서로 변환합니다\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 단어 하나씩 예측해 문장을 만듭니다\n",
    "    #    1. 입력받은 문장의 텐서를 입력합니다\n",
    "    #    2. 예측된 값 중 가장 높은 확률인 word index를 뽑아냅니다\n",
    "    #    3. 2에서 예측된 word index를 문장 뒤에 붙입니다\n",
    "    #    4. 모델이 <end>를 예측했거나, max_len에 도달했다면 문장 생성을 마칩니다\n",
    "    while True:\n",
    "        # 1\n",
    "        predict = model(test_tensor) \n",
    "        # 2\n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
    "        # 3 \n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "        # 4\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # tokenizer를 이용해 word index를 단어로 하나씩 변환합니다 \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "803fdfd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> he s got a big ego hahaha <end> '"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#문장생성\n",
    "generate_text(model,tokenizer, init_sentence=\"<start> he\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6f71d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc55da0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
