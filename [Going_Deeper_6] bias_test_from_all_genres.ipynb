{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[Going_Deeper_6]bias_test_from_all_genres",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1UxsMACn7X4jeV5_muvhICMTeiO0gh_fw",
      "authorship_tag": "ABX9TyP6fBZNcd3mkwW7rJIeDiHR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gelgel5633/exploration/blob/main/%5BGoing_Deeper_6%5D%20bias_test_from_all_genres.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install konlpy"
      ],
      "metadata": {
        "id": "OzxO2rjrk0XO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FT36IaQHjNaD"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numpy import dot\n",
        "from numpy.linalg import norm\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "from gensim.models import KeyedVectors\n",
        "from konlpy.tag import Okt\n",
        "import os\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import seaborn as sns\n",
        "\n",
        "plt.rc('font', family='NanumBarunGothic') "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#코사인 유사도 구하는 함수\n",
        "def cos_sim(i, j):\n",
        "    return dot(i, j.T)/(norm(i)*norm(j))\n",
        "\n",
        "#weat_score의 분자를 구하는 함수\n",
        "def s(w, A, B):\n",
        "    c_a = cos_sim(w, A)\n",
        "    c_b = cos_sim(w, B)\n",
        "    mean_A = np.mean(c_a, axis=-1)\n",
        "    mean_B = np.mean(c_b, axis=-1)\n",
        "    return mean_A - mean_B #, c_a, c_b\n",
        "\n",
        "#weat_score 구하는 함수\n",
        "def weat_score(X, Y, A, B):\n",
        "    \n",
        "    s_X = s(X, A, B)\n",
        "    s_Y = s(Y, A, B)\n",
        "\n",
        "    mean_X = np.mean(s_X)\n",
        "    mean_Y = np.mean(s_Y)\n",
        "    \n",
        "    std_dev = np.std(np.concatenate([s_X, s_Y], axis=0))\n",
        "    \n",
        "    return  (mean_X-mean_Y)/std_dev"
      ],
      "metadata": {
        "id": "MvBiTxNhku4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 형태소 분석기를 이용하여 품사가 명사인 경우 해당 단어를 추출하기"
      ],
      "metadata": {
        "id": "tYOvol87ve_e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "okt = Okt()\n",
        "tokenized = []\n",
        "#synopsis.txt에는 2001년~2019년 8월까지 영화의 시놉시스 존재\n",
        "with open('/content/drive/MyDrive/test/data/synopsis.txt', 'r') as file:\n",
        "    while True:\n",
        "        line = file.readline()\n",
        "        if not line: break\n",
        "        #okt.pos를 통해 품사를 태딩하고 아래 for문에 적용\n",
        "        words = okt.pos(line, stem=True, norm=True)\n",
        "        res = []\n",
        "        for w in words:\n",
        "            if w[1] in [\"Noun\"]:      # 품사가 명사일 떄\n",
        "                res.append(w[0])    # tokenized 에 저장한다.\n",
        "        tokenized.append(res)\n",
        "\n",
        "print(len(tokenized))"
      ],
      "metadata": {
        "id": "lHXKaQZBk6Yv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 추출된 결과로 embedding model 만들기"
      ],
      "metadata": {
        "id": "GmUZ_MPKvmdN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "model = Word2Vec(tokenized, size=300, window=5, min_count=3, sg=0)\n",
        "#테스트 결과\n",
        "#매개변수 topn을 5로 설정하고 상위 5개 항목만 출력\n",
        "print(model.most_similar(positive=['공포'], topn=5))\n",
        "print(\"-------------------------------------------------------\")\n",
        "print(model.most_similar(positive=['스릴'], topn=5))"
      ],
      "metadata": {
        "id": "3xgDvKYqmZx1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "size를 100으로 하였을 때는 스릴 항목에서 등장인물의 이름이 다수 나왔었는데, 사이즈를 키움으로써 이를 해소할 수 있었다. 다만, 여전히 프란츠(영화이름 or 등장인물)이 나오는 것으로 봤을 때 의구심이 든다."
      ],
      "metadata": {
        "id": "-Yj6yCSYyvbj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# target, attribute 단어 셋 만들기"
      ],
      "metadata": {
        "id": "n3kagNgezL7R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "target, attribute는 각각 weat score를 구하기 위하여 만들어 주어야 한다. "
      ],
      "metadata": {
        "id": "XJvH7qXczNnf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "art_txt = 'synopsis_art.txt'\n",
        "gen_txt = 'synopsis_gen.txt'\n",
        "\n",
        "def read_token(file_name):\n",
        "    okt = Okt()\n",
        "    result = []\n",
        "    with open('/content/drive/MyDrive/test/data/'+file_name, 'r') as fread: \n",
        "        print(file_name, '파일을 읽는 중')\n",
        "        while True:\n",
        "            line = fread.readline() \n",
        "            if not line: break \n",
        "            tokenlist = okt.pos(line, stem=True, norm=True) \n",
        "            for word in tokenlist:\n",
        "                if word[1] in [\"Noun\"]:#, \"Adjective\", \"Verb\"]:\n",
        "                    result.append((word[0])) \n",
        "    return ' '.join(result)"
      ],
      "metadata": {
        "id": "eCmzSDxRwEtJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#art와 gen는 예술영화와 상업영화의 장르를 가르는 target\n",
        "art = read_token(art_txt)\n",
        "gen = read_token(gen_txt)"
      ],
      "metadata": {
        "id": "B5z8owhSzq9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#오류 방지를 위한 복사본 생성\n",
        "art_copy = art\n",
        "gen_copy = gen"
      ],
      "metadata": {
        "id": "ylg_NcWcz6ht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tfidf vectorizer를 사용하여 단어셋 구성\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform([art_copy, gen_copy])"
      ],
      "metadata": {
        "id": "KWyxb6ZA1BtP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#coo 행렬로 변환\n",
        "m1 = X[0].tocoo() # art를 TF-IDF로 표현한 sparse matrix\n",
        "m2 = X[1].tocoo() # gen을 TF-IDF로 표현한 sparse matrix\n",
        "\n",
        "#중복제거를 위해 1열과 2열에서 0의 값을 삭제\n",
        "m1.eliminate_zeros()\n",
        "m2.eliminate_zeros()"
      ],
      "metadata": {
        "id": "hKpKc6Zn2C_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w1 = [[i, j] for i, j in zip(m1.col, m1.data)]\n",
        "w2 = [[i, j] for i, j in zip(m2.col, m2.data)]\n",
        "\n",
        "w1.sort(key=lambda x: x[1], reverse=True)   #art를 구성하는 단어들을 TF-IDF가 높은 순으로 정렬합니다. \n",
        "w2.sort(key=lambda x: x[1], reverse=True)   #gen을 구성하는 단어들을 TF-IDF가 높은 순으로 정렬합니다. \n",
        "\n",
        "print('예술영화를 대표하는 단어들:')\n",
        "for i in range(40):\n",
        "    print(vectorizer.get_feature_names()[w1[i][0]], end=', ')\n",
        "\n",
        "print('\\n')\n",
        "    \n",
        "print('일반영화를 대표하는 단어들:')\n",
        "for i in range(40):\n",
        "    print(vectorizer.get_feature_names()[w2[i][0]], end=', ')"
      ],
      "metadata": {
        "id": "HG8F3mu49cUC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = 20\n",
        "w1_, w2_ = [], []\n",
        "for i in range(100):\n",
        "    w1_.append(vectorizer.get_feature_names()[w1[i][0]])\n",
        "    w2_.append(vectorizer.get_feature_names()[w2[i][0]])\n",
        "\n",
        "# w1에만 있고 w2에는 없는, 예술영화를 잘 대표하는 단어를 n개 추출\n",
        "target_art, target_gen = [], []\n",
        "for i in range(100):\n",
        "    if (w1_[i] not in w2_) and (w1_[i] in model.wv): target_art.append(w1_[i])\n",
        "    if len(target_art) == n: break \n",
        "\n",
        "# w2에만 있고 w1에는 없는, 일반영화를 잘 대표하는 단어를 n개 추출\n",
        "for i in range(100):\n",
        "    if (w2_[i] not in w1_) and (w2_[i] in model.wv): target_gen.append(w2_[i])\n",
        "    if len(target_gen) == n: break"
      ],
      "metadata": {
        "id": "ygeFGoxm2Dab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(target_art)\n",
        "print(target_gen)"
      ],
      "metadata": {
        "id": "MyD8V9mt7LB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "genre_txt = ['synopsis_SF.txt', 'synopsis_family.txt', 'synopsis_show.txt', 'synopsis_horror.txt', 'synopsis_etc.txt', \n",
        "             'synopsis_documentary.txt', 'synopsis_drama.txt', 'synopsis_romance.txt', 'synopsis_musical.txt', \n",
        "             'synopsis_mystery.txt', 'synopsis_crime.txt', 'synopsis_historical.txt', 'synopsis_western.txt', \n",
        "             'synopsis_adult.txt', 'synopsis_thriller.txt', 'synopsis_animation.txt', 'synopsis_action.txt', \n",
        "             'synopsis_adventure.txt', 'synopsis_war.txt', 'synopsis_comedy.txt', 'synopsis_fantasy.txt']\n",
        "genre_name = ['SF', '가족', '공연', '공포(호러)', '기타', '다큐멘터리', '드라마', '멜로로맨스', '뮤지컬', '미스터리', '범죄', '사극', '서부극(웨스턴)',\n",
        "         '성인물(에로)', '스릴러', '애니메이션', '액션', '어드벤처', '전쟁', '코미디', '판타지']"
      ],
      "metadata": {
        "id": "1svlCI_9702-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#genre = attribute 역할을 함. weat_score에서 \n",
        "genre = []\n",
        "for file_name in genre_txt:\n",
        "    genre.append(read_token(file_name))"
      ],
      "metadata": {
        "id": "y4jue7yI_OhL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(genre)"
      ],
      "metadata": {
        "id": "qsPbAjEh_gu-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(genre)\n",
        "\n",
        "print(X.shape)"
      ],
      "metadata": {
        "id": "eBcHeOm__jba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 각 장르 대표단어 추출\n",
        "m = [X[i].tocoo() for i in range(X.shape[0])]\n",
        "\n",
        "w = [[[i, j] for i, j in zip(mm.col, mm.data)] for mm in m]\n",
        "\n",
        "for i in range(len(w)):\n",
        "    w[i].sort(key=lambda x: x[1], reverse=True)\n",
        "attributes = []\n",
        "for i in range(len(w)):\n",
        "    print(genre_name[i], end=': ')\n",
        "    attr = []\n",
        "    j = 0\n",
        "    while (len(attr) < 15):\n",
        "        if vectorizer.get_feature_names()[w[i][j][0]] in model.wv:\n",
        "            attr.append(vectorizer.get_feature_names()[w[i][j][0]])\n",
        "            print(vectorizer.get_feature_names()[w[i][j][0]], end=', ')\n",
        "        j += 1\n",
        "    attributes.append(attr)\n",
        "    print()"
      ],
      "metadata": {
        "id": "T2KT31aJBIOX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WEAT score 계산과 시각화"
      ],
      "metadata": {
        "id": "ldVw7eYqEZOh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#embedding model과 단어셋으로 weat score 구하기\n",
        "#구한 결과를 21*21 매트릭스형태로 표현\n",
        "matrix = [[0 for _ in range(len(genre_name))] for _ in range(len(genre_name))]"
      ],
      "metadata": {
        "id": "slwur8oSERhf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array([model.wv[word] for word in target_art])\n",
        "Y = np.array([model.wv[word] for word in target_gen])\n",
        "\n",
        "for i in range(len(genre_name)-1):\n",
        "    for j in range(i+1, len(genre_name)):\n",
        "        A = np.array([model.wv[word] for word in attributes[i]])\n",
        "        B = np.array([model.wv[word] for word in attributes[j]])\n",
        "        matrix[i][j] = weat_score(X, Y, A, B)"
      ],
      "metadata": {
        "id": "i04JDH8YEs47"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#weat score 값 확인\n",
        "for i in range(len(genre_name)-1):\n",
        "    for j in range(i+1, len(genre_name)):\n",
        "        #절대값이 0.9이상인 것들만 출력\n",
        "        if abs(matrix[i][j]) >= 0.9:\n",
        "            print(genre_name[i], genre_name[j],matrix[i][j])\n",
        "        else:\n",
        "            continue"
      ],
      "metadata": {
        "id": "NjTctDD_Ey-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 결과 해석\n",
        "* weat 스코어는 예술영화에서 상업영화를 뺀 값으로 설정 ( 함수는 최상단 위치)\n",
        "* 위는 상관관계가 절대값 90%이상인 값을 가지고 왔음. 더 세부적인 결과를 보려면 절대값 80%이상의 값을 가지고 와야할 것으로 보임\n",
        "* 양수일수록 예술영화, 음수일수록 상업영화(=일반영화)에 가까운 값이 됨\n",
        "* 가족, 공연, 드라마, 멜로, 미스터리는 예술영화에 가깝지만, 기타, 다큐멘터리, 범죄 등은 상업(일반)영화와 가까운 것으로 나타났다."
      ],
      "metadata": {
        "id": "ueUWtKRNG79t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#코랩 코드깨짐문제 해결을 위해설치\n",
        "!sudo apt-get install -y fonts-nanum\n",
        "!sudo fc-cache -fv\n",
        "!rm ~/.cache/matplotlib -rf"
      ],
      "metadata": {
        "id": "65rYSD5uRG6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(0)\n",
        "\n",
        "# 한글 지원 폰트\n",
        "sns.set(font='NanumBarunGothic')\n",
        "\n",
        "# 마이너스 부호 \n",
        "plt.rcParams['axes.unicode_minus'] = False\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(15,15))\n",
        "\n",
        "ax = sns.heatmap(matrix, xticklabels=genre_name, yticklabels=genre_name, annot=True,  cmap='RdYlGn_r')\n",
        "ax"
      ],
      "metadata": {
        "id": "E4POrsX5FPX8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 회고\n",
        "* tfidf 행렬을 그대로 사용해도 되는데, 중복을 제거하라고 했을 때 정말 막막했었다. 찾아도 안나오기에 포기하려다가 검색어를 바꾸니 쉽게 쓸 수 있는 함수가 나왔고, 바로 적용시켰다.  \n",
        "* 오타는 제외인줄 알았는데 포함이었다. 오타를 많이 내서 이런 결과가 나타난 것인데, 이것을 수정하여 넣는 법이 있으면 좋겠지만, 오타를 그대로 유지한다면 조금 더 robust한 결과가 나오지 않을까란 생각이 든다.\n",
        "* 결과가 의문스러운 점이 있다. 드라마와 미스터리는 일반영화, 다큐멘터리는 예술영화에 가깝다고 생각하는데, 분류는 반대로 되었다. 중복 제거가 잘못되었나...?\n",
        "* 예술영화, 일반영화의 경계선이 모호해서 예상했던 결과가 나오지 않았을 수도 있을 것 같다. 누가 답지를 펴고 보여줬으면 좋겠다.\n",
        "* 아니면 target값에만 중복제거를 해준것이 문제일 수도 있을 것 같다. attribute도 삭제하는게 나았을지도 모른다.\n",
        "* weat score는 처음들어보는 단어라 이해하는데 시간을 많이 사용했다. 빠르게 변화하는 분야인만큼 이해하는 속도가 증가역시 필요해보인다"
      ],
      "metadata": {
        "id": "h65mZBRIUJs8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "pdMrL2mZVXO1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}